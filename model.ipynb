{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self):\n",
    "        self.seq_length = 101\n",
    "        self.data_path = \"with_reverse/RBBP5\"\n",
    "        self.negative_list = []\n",
    "        self.positive_list = []\n",
    "        self.positivecount = 0\n",
    "        self.negativecount = 0\n",
    "        self.onehot_encoded_positive = []\n",
    "        self.onehot_encoded_negative = []\n",
    "        \n",
    "    def onehot_encode_sequences(self, sequences):\n",
    "        onehot = []\n",
    "        mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'U': 3, 'N':4}\n",
    "        for sequence in sequences:\n",
    "            arr = np.zeros((len(sequence), 5)).astype(\"float\")\n",
    "            for (i, letter) in enumerate(sequence):\n",
    "                arr[i, mapping[letter]] = 1.0\n",
    "            onehot.append(arr)\n",
    "        return onehot\n",
    "\n",
    "    def reverse_complement(self, dna):\n",
    "        complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A','N': 'N'}\n",
    "        return ''.join([complement[base] for base in dna[::-1]])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fasta_prepare_data(self):\n",
    "        path = os.path.join(self.data_path, \"RBBP5_positive_training.fa\")\n",
    "        f = open(path, 'r')\n",
    "        for line in f:\n",
    "            self.positive_list.append(line)\n",
    "            self.positivecount += 1\n",
    "        #print(self.positivecount)\n",
    "        self.positive_list = list(map(lambda x:x.strip(), self.positive_list))\n",
    "        onehot_positive = self.onehot_encode_sequences(self.positive_list)\n",
    "\n",
    "\n",
    "        path = os.path.join(self.data_path, \"RBBP5_negative_training.fa\")\n",
    "        f = open(path, 'r')\n",
    "        for line in f:\n",
    "            self.negative_list.append(line)\n",
    "            self.negativecount += 1\n",
    "        #print(self.negativecount)\n",
    "        self.negative_list = list(map(lambda x:x.strip(), self.negative_list))\n",
    "        onehot_negative = self.onehot_encode_sequences(self.negative_list)\n",
    "        \n",
    "        combinedlist = []\n",
    "        \n",
    "        for sequence in onehot_positive:\n",
    "            combinedlist.append([sequence, [0,1]])\n",
    "        for sequence in onehot_negative:\n",
    "            combinedlist.append([sequence,[1,0]])\n",
    "        \n",
    "        np.random.shuffle(combinedlist)\n",
    "        \n",
    "        \n",
    "        for data in combinedlist:\n",
    "            data[0] = np.transpose(data[0])\n",
    "            \n",
    "        \n",
    "            \n",
    "        return combinedlist\n",
    "    \n",
    "    \n",
    "\n",
    "    def seq_prepare_data(self):\n",
    "        #----------------------------------------------------------------------------------------------\n",
    "        # positive data\n",
    "        path = \"/usr/local/lib/EzGeno/Backbone_model_data/\"\n",
    "        file_counter = 0\n",
    "        \n",
    "        for file in os.listdir(path):\n",
    "            new_path = os.path.join(path, file)\n",
    "\n",
    "\n",
    "            if new_path.lower().endswith('seq'):\n",
    "                file_counter += 1\n",
    "                #print(\"Number of files processed: %s\" %(file_counter))\n",
    "                # print(new_path)\n",
    "                with open(new_path) as outputfile:\n",
    "                    for sequence in outputfile:\n",
    "\n",
    "                        if sequence[0] != 'A':\n",
    "                            pass\n",
    "                        else:\n",
    "                            self.positive_list.append(sequence[17:118])\n",
    "                            self.positive_list.append(self.reverse_complement(sequence[17:118]))\n",
    "\n",
    "        print(\"shuffling\")                    \n",
    "        random.shuffle(self.positive_list)\n",
    "        print(\"Number of positive sequences: %s\" %(len(self.positive_list)))\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------\n",
    "        #negative data\n",
    "\n",
    "        negative_path = \"/usr/local/lib/EzGeno/negative_fasta.fa\"\n",
    "        test_sequences = SeqIO.parse(open('negative_fasta.fa'),'fasta')\n",
    "        with open('negative_fasta.fa') as out_file:\n",
    "            for sequence in test_sequences:\n",
    "                self.negative_list.append(str(sequence.seq).upper())\n",
    "\n",
    "        print(\"Number of negative sequences: %s\" %(len(self.negative_list)))\n",
    "        \n",
    "        \n",
    "        return self.positive_list, self.negative_list\n",
    "\n",
    "        \n",
    "        \n",
    "my_data = Data()\n",
    "data1 = my_data.fasta_prepare_data()\n",
    "#positive_list, negative_list = my_data.seq_prepare_data()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------parse to 10000 sequences per file----------------------\n",
    "\n",
    "file_counter = 1\n",
    "for i in range(0, len(positive_list), 10000):\n",
    "    pos_filename = \"processed/positive/positive_%d.fa\" % file_counter\n",
    "    neg_filename = \"processed/negative/negative_%d.fa\" % file_counter\n",
    "    with open(pos_filename, 'w')as f:\n",
    "        for item in positive_list[i:i + 10000]:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    with open(neg_filename, 'w')as f:\n",
    "        for item in negative_list[i:i + 10000]:\n",
    "            f.write(\"%s\\n\" % item)  \n",
    "    file_counter += 1\n",
    "    \n",
    "#---------------parse to 10000 sequences per file---------------------   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------Dataset Class-----------------------------------\n",
    "\n",
    "class singlefile(data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, data_root, filename):\n",
    "        \n",
    "        self.path = os.path.join(data_root, filename)\n",
    "        self.xy = []\n",
    "        \n",
    "        f = open(self.path, 'r')\n",
    "        for line in f:\n",
    "            if 'positive' in data_root:\n",
    "                temp_list = self.one_hot(line.strip('\\n'))\n",
    "                self.xy.append([np.transpose(torch.Tensor(temp_list)), torch.Tensor([0,1])])\n",
    "            if 'negative' in data_root:\n",
    "                temp_list = self.one_hot(line.strip('\\n'))\n",
    "                self.xy.append([np.transpose(torch.Tensor(temp_list)), torch.Tensor([1,0])])\n",
    "\n",
    "        \n",
    "        \n",
    "    def one_hot(self, seq):\n",
    "        encoding_map = {'A': [1,0,0,0,0], 'T': [0,1,0,0,0], 'C': [0,0,1,0,0], 'G': [0,0,0,1,0], 'N': [0,0,0,0,1]}\n",
    "        temp_list = []\n",
    "        for s in seq:\n",
    "            temp_list.append(encoding_map[s])\n",
    "        return temp_list\n",
    "               \n",
    "    def __getitem__(self, index):\n",
    "        return self.xy[index][0], self.xy[index][1]\n",
    "        #return self.xy[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xy)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "#test = singlefile('processed/positive', 'positive_500.fa')\n",
    "#print(test.__getitem__(3))\n",
    "\n",
    "\n",
    "listofdatasets = []\n",
    "for i, j in zip(os.listdir('processed/positive'), os.listdir('processed/negative')):\n",
    "    #listofdatasets = []\n",
    "    if not i.endswith('.fa'):\n",
    "        continue\n",
    "    if not j.endswith('.fa'):\n",
    "        continue\n",
    "        \n",
    "    listofdatasets.append(singlefile('processed/positive', i))\n",
    "    listofdatasets.append(singlefile('processed/negative', j))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #break\n",
    "    \n",
    "random.shuffle(listofdatasets)\n",
    "concat_dataset = data.ConcatDataset(listofdatasets)\n",
    "\n",
    "print('saving')\n",
    "with open('concat_dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(concat_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "\n",
    "print(len(concat_dataset))\n",
    "#print(concat_dataset[1999].__getitem__(1))\n",
    "#print(concat_dataset[1999].__getitem__(0))\n",
    "\n",
    "\n",
    "#---------------------------Dataset Class-----------------------------------   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "19999\n"
     ]
    }
   ],
   "source": [
    "class allfiles(data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, pos_data_dir, neg_data_dir):\n",
    "        \n",
    "        self.list_index_counter = 0\n",
    "        self.combined_list = []\n",
    "        self.pos_filename = os.listdir(pos_data_dir)\n",
    "        self.neg_filename = os.listdir(neg_data_dir)\n",
    "        self.num_files = len(self.pos_filename)\n",
    "        self.pos_filepath = os.path.join(pos_data_dir, self.pos_filename[self.list_index_counter]) \n",
    "        self.neg_filepath = os.path.join(neg_data_dir, self.neg_filename[self.list_index_counter])\n",
    "        \n",
    "        f = open(self.pos_filepath, 'r')\n",
    "        for line in f:\n",
    "            temp_seq = self.one_hot(line.strip('\\n'))\n",
    "            self.combined_list.append([np.transpose(torch.Tensor(temp_seq)), torch.Tensor([0,1])])\n",
    "            \n",
    "        f = open(self.neg_filepath, 'r') \n",
    "        for line in f:\n",
    "            temp_seq = self.one_hot(line.strip('\\n'))\n",
    "            self.combined_list.append([np.transpose(torch.Tensor(temp_seq)), torch.Tensor([1,0])])\n",
    "        \n",
    "        self.list_index_counter += 1    \n",
    "        random.shuffle(self.combined_list)        \n",
    "        \n",
    "    def one_hot(self, seq):\n",
    "        encoding_map = {'A': [1,0,0,0,0], 'T': [0,1,0,0,0], 'C': [0,0,1,0,0], 'G': [0,0,0,1,0], 'N': [0,0,0,0,1]}\n",
    "        temp_list = []\n",
    "        for s in seq:\n",
    "            temp_list.append(encoding_map[s])\n",
    "        return temp_list\n",
    "               \n",
    "    def __getitem__(self, index):\n",
    "        if self.list_index_counter < self.num_files:\n",
    "            if len(self.combined_list) < 15000:\n",
    "                self.pos_filepath = os.path.join(pos_data_dir, self.pos_filename[self.list_index_counter]) \n",
    "                self.neg_filepath = os.path.join(neg_data_dir, self.neg_filename[self.list_index_counter])\n",
    "                f = open(self.pos_filepath, 'r')\n",
    "                for line in f:\n",
    "                    temp_seq = self.one_hot(line.strip('\\n'))\n",
    "                    self.combined_list.append([np.transpose(torch.Tensor(temp_seq)), torch.Tensor([0,1])])\n",
    "\n",
    "                f = open(self.neg_filepath, 'r') \n",
    "                for line in f:\n",
    "                    temp_seq = self.one_hot(line.strip('\\n'))\n",
    "                    self.combined_list.append([np.transpose(torch.Tensor(temp_seq)), torch.Tensor([1,0])])\n",
    "                self.list_index_counter += 1\n",
    "            \n",
    "            \n",
    "        random.shuffle(self.combined_list) \n",
    "        selected_index = self.combined_list[index]\n",
    "        self.combined_list.pop(index)\n",
    "         \n",
    "        return selected_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.combined_list)\n",
    "    \n",
    "test = allfiles('processed/positive', 'processed/negative')\n",
    "print(len(test.combined_list))\n",
    "test.__getitem__(5)\n",
    "print(len(test.combined_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Model Class----------------------------\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(5, 16, 11, stride=1, padding=5)\n",
    "        self.conv2 = nn.Conv1d(16, 16, 11, stride=1, padding=5)\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.conv3 = nn.Conv1d(16, 16, 11, stride=1, padding=5)\n",
    "        self.conv4 = nn.Conv1d(16, 16, 11, stride=1, padding=5)\n",
    "        self.drop2 = nn.Dropout(0.25)\n",
    "        self.conv5 = nn.Conv1d(16, 16, 11, stride=1, padding=5)\n",
    "        self.conv6 = nn.Conv1d(16, 16, 11, stride=1, padding=5)\n",
    "        self.drop3 = nn.Dropout(0.25)\n",
    "        \n",
    "    \n",
    "        x = torch.randn((5, 101)).view(-1, 5 , 101)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 500)\n",
    "        self.fc2 = nn.Linear(500,2)\n",
    "        \n",
    "    def convs(self, x):\n",
    "        \n",
    "        x = F.max_pool1d(F.relu(self.conv2(F.relu(self.conv1(x)))), 2)\n",
    "        x = (self.drop1(x))\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = F.max_pool1d(F.relu(self.conv4(F.relu(self.conv3(x)))),  2)\n",
    "        x = (self.drop2(x))\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = F.max_pool1d(F.relu(self.conv6(F.relu(self.conv5(x)))),  2)\n",
    "        x = (self.drop3(x))\n",
    "        #print(x.shape)\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "           # print(x[0].shape)\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]\n",
    "            #print(self._to_linear)\n",
    "        return x\n",
    "        \n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "#-----------------------Model Class----------------------------     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(5, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (conv2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      "  (conv3): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (conv4): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (drop2): Dropout(p=0.25, inplace=False)\n",
      "  (conv5): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (conv6): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (drop3): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=192, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#------initialize model and hyperparameters--------\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "batch_size = 64\n",
    "EPOCHS = 30\n",
    "\n",
    "#------initialize model and hyperparameters--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------initialize data for single fasta--------------\n",
    "\n",
    "x = torch.Tensor([i[0] for i in data1]).view(-1, 5, 101)\n",
    "y = torch.Tensor([i[1] for i in data1])\n",
    "\n",
    "#--------initialize data for single fasta--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------training for single fasta------------------\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(x), batch_size)):\n",
    "        batchx = x[i : i+batch_size].view(-1, 5, 101)\n",
    "        batchy = y[i : i+batch_size]\n",
    "\n",
    "\n",
    "        net.zero_grad()\n",
    "        \n",
    "        outputs = net.forward(batchx)\n",
    "        loss = loss_function(outputs, batchy)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "    \n",
    "#----------------training for single fasta------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.6560514569282532\n",
      "Train AUC score: 0.7095\n",
      "Accuracy:  0.70945\n",
      "Epoch: 2. Loss: 0.45818066596984863\n",
      "Train AUC score: 0.7767\n",
      "Accuracy:  0.7767\n",
      "Epoch: 3. Loss: 0.30576083064079285\n",
      "Train AUC score: 0.7950\n",
      "Accuracy:  0.795\n",
      "Epoch: 4. Loss: 0.3440389037132263\n",
      "Train AUC score: 0.8059\n",
      "Accuracy:  0.80595\n",
      "Epoch: 5. Loss: 0.5110734701156616\n",
      "Train AUC score: 0.8127\n",
      "Accuracy:  0.8127\n",
      "Epoch: 6. Loss: 0.2087555229663849\n",
      "Train AUC score: 0.8152\n",
      "Accuracy:  0.8152\n",
      "Epoch: 7. Loss: 0.4304315745830536\n",
      "Train AUC score: 0.8188\n",
      "Accuracy:  0.8188\n",
      "Epoch: 8. Loss: 0.24490857124328613\n",
      "Train AUC score: 0.8227\n",
      "Accuracy:  0.8227\n",
      "Epoch: 9. Loss: 0.5164089202880859\n",
      "Train AUC score: 0.8283\n",
      "Accuracy:  0.82835\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c0458d97ef25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-67925b5ea5e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_linear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-67925b5ea5e4>\u001b[0m in \u001b[0;36mconvs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#----------------------------Model Training--------------------------------\n",
    "\n",
    "seqdataloader = data.DataLoader(concat_dataset, batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    correct = 0\n",
    "    all_label = []\n",
    "    all_pred = []\n",
    "    \n",
    "    for i, (inputs, label) in enumerate(seqdataloader):\n",
    "        \n",
    "        \n",
    "        inputs = inputs.view(-1, 5, 101)\n",
    "        \n",
    "        #print(inputs)\n",
    "        #inputs = torch.Tensor(inputs[0])\n",
    "        #print(len(inputs[0][0]))\n",
    "        #inputs = inputs.view(-1, 5, 101)\n",
    "        \n",
    "        #print(label)\n",
    "        net.zero_grad()\n",
    "        outputs = net.forward(inputs)\n",
    "        #print(outputs)\n",
    "        loss = loss_function(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #---------------------AUC------------------------\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        real_labels = label.argmax(dim=1, keepdim= True)\n",
    "        \n",
    "        #for i in range(len(real_labels)):\n",
    "            #print(real_labels[i], pred[i])\n",
    "            \n",
    "        correct += pred.eq(real_labels).sum().item()\n",
    "        #print(correct)\n",
    "        all_label.extend(real_labels.tolist())\n",
    "        all_pred.extend(pred.tolist())\n",
    "        #print(len(all_label))\n",
    "        #------------------------------------------------\n",
    "    #print(correct)\n",
    "    #print(len(all_label))\n",
    "    #print(correct/len(all_label))\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "    print(\"Train AUC score: {:.4f}\".format(roc_auc_score(np.array(all_label), np.array(all_pred))))\n",
    "    acc = correct/len(all_label)\n",
    "    print('Accuracy: ' ,correct/len(all_label))\n",
    "         \n",
    "#----------------------------Model Training--------------------------------    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
